\section{Benchmarks}
To measure the perfomance of our Advert service solution, we performed
benchmarks on our Google App Engine Advert Service library. We executed a
couple of tests to measure latency, bandwidth, and server-side performance.
Below we describe our benchmarks in more detail.

All measurements below were perfomed on a 2.16 GHz Intel Core 2 Duo iMac, with
1GB 667 MHz DDR2 SDRAM (unless stated otherwise). Network performance may vary,
depending on your bandwidth and network traffic. We did our measurements from
the \emph{Vrije Universiteit}, which owns a fibreglass internet connection
provided by \emph{SURFnet} (unless stated otherwise).

Furthermore, we used the UNIX \texttt{traceroute} command to determine where our
App Engine application is hosted. The results can be found in figure
\ref{tracert}. The last known IP address resides at Mountain View, California
(US), which is where Google Inc. is residing. It is safe to assume that all our
request go to Google US and back.

\begin{figure*}[ht] %[placement] where placement is h,t,b,p
\begin{center}
\begin{code}
$ traceroute ibisadvert.appspot.com
traceroute to appspot.l.google.com (74.125.79.141), 64 hops max, 40 byte packets
 1  router-student1 (130.37.24.7)  0.641 ms  0.314 ms  0.293 ms
 2  hkae16-2-d02.backbone.vu.nl (130.37.5.54)  0.288 ms  0.257 ms  0.274 ms
 3  GE5-1-1.2090.JNR01.Asd002A.surf.net (145.145.20.57)  0.674 ms  0.665 ms  0.620 ms
 4  AE0.500.JNR02.Asd002A.surf.net (145.145.80.65)  0.690 ms  0.657 ms  0.745 ms
 5  core1.ams.net.google.com (195.69.144.247)  1.156 ms  0.993 ms  0.997 ms
 6  209.85.248.93  11.159 ms  1.353 ms  1.269 ms
 7  64.233.175.246  14.791 ms 72.14.233.114  6.209 ms 64.233.175.246  4.269 ms 
 8  72.14.239.199  5.749 ms 209.85.255.166  5.932 ms 72.14.239.197  4.606 ms 
 9  209.85.255.126  7.864 ms 209.85.255.122  6.025 ms  6.676 ms 
 10  * * *
\end{code}
\caption{Traceroute output of ibisadvert.appspot.com.\label{tracert}}
\end{center}
\end{figure*}

\subsection{Initialization}
Initializing our Advert library can be done in two different ways. First, we
have the public Advert server, which does not require any authentication 
whatsoever. Initializing this class takes a negligible amount of time (i.e.
less than 0ms).

Secondly, we have a private server model, which does require authentication.
Authenticating to a private Advert server can be divided in three parts:

\begin{itemize}
  \item Authenticating to Google using ClientLogin
  \item Retrieving a authentication cookie from the Google App Engine
  \item Initializing and starting up `Persistent Authentication' thread
\end{itemize}

We measured the total time of initializing the Advert library (client-side),
which has an average of 190ms (179ms minimum, 229ms maximum). In addition we
measured each of the parts above individually, after which we can conclude that
authenticating using ClientLogin takes up one-third of the latency stated above,
and retriving the authentication cookie takes up two-third of the time measured
above. Initializing and starting up our `Persistent Authentication' thread
takes a negligible amount of time (i.e. less than 0ms).

Note that it is impossible to measure server-side performance/latency, because
ClientLogin and the process of retrieving a authentication cookie from the
Google App Engine, are both closed source procedures at Google, in which we
cannot place any timers.

As from now, all measurements will be done with respect to the authenticated
server, because we expect it will be used most in practice.

\subsection{Client Functions}
Another interesting thing to measure is the latency generated by various client
functions, and how this latency increases when we process greater amounts data.
We conducted measurements of calling all client functions with variable amounts
of data. Results can be viewed below:

\subsubsection{Add}
The process of adding an object to the datastore consists of four parts.

\begin{itemize}
  \item Processing the object (encoding to Base64 and JSON) and meta data
  \item Sending the object to the advert server
  \item Storing (possibly overwriting) the object and meta data
  \item Receiving a response
\end{itemize}

Notably, latency will increase as data-to-process increases. That's why we
conducted measurements with a variable object and meta data size. Note that
Google's datastore API has a maximum call size of 1MB, and since we are
encoding all our objects in Base64, the objects to add should be roughly 1.4
times smaller than what we really want to store. Therefore, we stored objects 
of size 730Bytes, 7.300Bytes, 73.000Bytes, and 730.000Bytes (being 1kB, 10kB,
100kB, and 1.000kB of data stored, respectively).

\paragraph{Variable Object Size}
Below we state the results of the measurements we did client-side. Note that
these measurements are network latency dependent. For network independent
measurements see the Server-side measurements. We added an object (without meta
data), of variable size, to the datastore ten times.\newline

\begin{figure}
\begin{tabular}{|r|r|r|r|}
\hline
Bytes sent & Min. & Avg. & Max. \\
\hline
730 & 222ms & 279ms & 449ms \\
7.300 & 221ms & 368ms & 815ms \\
73.000 & 511ms & 786ms & 952ms \\ 
730.000 & 1632ms & 2258ms & 5597ms \\
\hline
\end{tabular}
\caption{Client}
\end{figure}

\begin{figure}
\begin{tabular}{|r|r|r|r|}
\hline
Bytes sent & Min. & Avg. & Max. \\
\hline
730 & 76ms & 117ms & 311ms \\
7.300 & 63ms & 98ms & 273ms \\
73.000 & 85ms & 117ms & 305ms \\
730.000 & 173ms & 213ms & 292ms \\
\hline
\end{tabular}
\caption{Server}
\end{figure}

\paragraph{Variable Meta Data Size}
Lorem Ipsum \ldots

\begin{figure}
\begin{tabular}{|r|r|r|r|}
\hline
\# Key-Value Pairs & Min. & Avg. & Max. \\
\hline
0 & 222ms & 279ms & 449ms \\
5 & 270ms & 405ms & 592ms \\
25 & 398ms & 489ms & 718ms\\
50 &  641ms & 767ms & 909ms \\
100 & 968ms & 1151ms & 1647ms\\
\hline
\end{tabular}
\caption{Client}
\end{figure}
\paragraph{Variable Meta Data Size}

\begin{figure}
\begin{tabular}{|r|r|r|r|}
\hline
# Key-Value Pairs & Min. & Avg. & Max. \\
\hline
0 & 76ms & 117ms & 311ms \\
5 & 132ms & 238ms & 451ms \\
25 & 257ms & 326ms & 574ms \\
50 & 491ms & 600ms & 724ms \\
100 & 818ms & 984ms & 1379ms \\
\hline
\end{tabular}
\caption{Server}
\end{figure}

\paragraph{Overwrite}
Note that two occasions can occur server-side: an entry does not exist yet; the
object is stored in the datstore. An object does already exits; the object is
overwritten. We tested the latency at the server side by sending 730.000Bytes
with and without any meta data to see how the latency increases (\textbf{wil ik
met meer sizes doen!}).

\begin{figure}
\begin{tabular}{|l|r|r|r|}
\hline
 & Min. & Avg. & Max. \\
\hline
No Overwrite & 173ms & 213ms & 292ms \\
Overwrite & 355ms & 425ms & 552ms \\
\hline
\end{tabular}
\caption{Server}
\end{figure}

% ClientLogin: 66
% Cookie: 121
% Thread: 0
% ClientLogin: 72
% Cookie: 120
% Thread: 0
% ClientLogin: 76
% Cookie: 117
% Thread: 1
% ClientLogin: 82
% Cookie: 117
% Thread: 0
% ClientLogin: 62
% Cookie: 112
% Thread: 0
% ClientLogin: 65
% Cookie: 124
% Thread: 0
% ClientLogin: 64
% Cookie: 127
% Thread: 0
% ClientLogin: 66
% Cookie: 116
% Thread: 0
% ClientLogin: 66
% Cookie: 117
% Thread: 0
% ClientLogin: 66
% Cookie: 116
% Thread: 0